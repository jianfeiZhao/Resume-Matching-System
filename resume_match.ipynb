{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resume_match.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMAr+w6CBYG2HT4/Nw2rvOg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jianfeiZhao/Resume-Matching-System/blob/master/resume_match.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvfKJiFJPkHB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0766dd4f-60af-483e-9e60-5a7385906df7"
      },
      "source": [
        "!pip install pyspark\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/67/5158f846202d7f012d1c9ca21c3549a58fd3c6707ae8ee823adcaca6473c/pyspark-3.0.2.tar.gz (204.8MB)\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 69kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 21.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186687 sha256=3e7a8151bd72c2ef174504108b44393c810e6310054dfdaed2daba632beca1a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/09/da/c1f2859bcc86375dc972c5b6af4881b3603269bcc4c9be5d16\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.2\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU1BFVy8I1I9"
      },
      "source": [
        "## Preprocessing the jobs data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G9Yj7TCHHCB"
      },
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# load data\n",
        "#df = pd.read_csv('./jobs_small.csv', encoding=\"latin-1\")\n",
        "df = pd.read_csv('/content/sample_data/resume_match/jobs.csv', encoding=\"utf-8\")\n",
        "#print(df.head())\n",
        "\n",
        "# text preprocessing\n",
        "REPLACE_BY_SPACE_RE = re.compile('[#+_/(){}!^?<>\"''*\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "match_regex = re.compile('\\d+')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# data cleaning\n",
        "def clean_text(text):\n",
        "    # change to lower-csae\n",
        "    text = str(text).lower()\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    # remove BAD_SYMBOLS_RE\n",
        "    text = BAD_SYMBOLS_RE.sub('', text)\n",
        "    text = match_regex.sub('', text)\n",
        "    # drop the stopwords\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) \n",
        "    return text\n",
        "\n",
        "# clean the desc field\n",
        "df['desc_clean'] = df['description'].apply(clean_text)\n",
        "df.drop(columns=['description', 'id'], inplace=True)\n",
        "\n",
        "for i in range(len(df)):\n",
        "  try:\n",
        "    if df['desc_clean'][i]=='nan' or df['desc_clean'][i]=='' or len(df['desc_clean'][i]) < 100:\n",
        "      df.drop(labels=i, inplace=True)\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "df.dropna(axis=0, inplace=True)\n",
        "df['id'] = [i for i in range(1, len(df)+1)]\n",
        "#print(df['desc_clean'])\n",
        "df.to_csv('./jobs_clean.csv')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIktgKA_I_dD"
      },
      "source": [
        "##load your CV and start matching from here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x0T-83svJ_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b60acf-7bf2-4f73-dbdd-04feb4a1d568"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "#from pyspark.ml.feature import NGram\n",
        "\n",
        "spark=SparkSession \\\n",
        "        .builder \\\n",
        "        .appName('tfidf_app') \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# read and clean the resume file\n",
        "f = open('/content/sample_data/resume_match/CV.txt', 'r')       ############# change resume dir here ####################\n",
        "text = f.read()\n",
        "text = clean_text(text)\n",
        "df = pd.read_csv('/content/sample_data/resume_match/jobs_clean.csv', encoding=\"utf-8\")\n",
        "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "df.loc[0] = ['resume', 0, 0, 0, text, 0]\n",
        "df.to_csv('./jobs_clean.csv')\n",
        "\n",
        "# load data\n",
        "df0 = spark.read.csv(\"./jobs_clean.csv\", header=True, multiLine=True, inferSchema=True)\n",
        "df1 = pd.read_csv('./jobs_clean.csv')\n",
        "\n",
        "#df0.show()\n",
        "print('Total number of jobs：',df0.count()-1)\n",
        "print('\\nthe number of each distinct job:\\n', df1.job.value_counts()[:-1])\n",
        "print('\\nThere are', len(df1.job.unique())-1, 'different kinds of jobs in the table.')\n",
        "\n",
        "# split the desc field\n",
        "tokenizer = Tokenizer(inputCol='desc_clean', outputCol='desc_words')\n",
        "df = tokenizer.transform(df0)\n",
        "#df.show()\n",
        "#df.select('desc_words').show(10)\n",
        "\n",
        "# compute TF-IDF\n",
        "hashingTF = HashingTF(inputCol='desc_words', outputCol='desc_words_tf')\n",
        "tf = hashingTF.transform(df).cache()\n",
        "idf = IDF(inputCol='desc_words_tf', outputCol='desc_words_tfidf').fit(tf)\n",
        "tfidf = idf.transform(tf).cache()\n",
        "#print('tfidf for each job:', tfidf.select('desc_words_tfidf').show(10,truncate=False))\n",
        "\n",
        "# data normalization\n",
        "from pyspark.ml.feature import Normalizer\n",
        "normalizer = Normalizer(inputCol=\"desc_words_tfidf\", outputCol=\"norm\")\n",
        "tfidf = normalizer.transform(tfidf)\n",
        "#tfidf.select(\"id\", \"norm\").show(6)\n",
        "\n",
        "# compute similarity between jobs and resume\n",
        "import pyspark.sql.functions as psf \n",
        "from pyspark.sql.types import DoubleType\n",
        "print('\\nCompute the similarity between jobs and resume...')\n",
        "dot_udf = psf.udf(lambda x,y: float(x.dot(y)), DoubleType()) # define dot-product function\n",
        "tfidf = tfidf.alias(\"a1\").join(tfidf.alias(\"a2\"), psf.col(\"a1.id\") == 0)\\\n",
        "        .select(\n",
        "            psf.col(\"a1.job\"),\n",
        "            psf.col(\"a1.id\").alias(\"id1\"), \n",
        "            psf.col(\"a2.id\").alias(\"id2\"), \n",
        "            dot_udf(\"a1.norm\", \"a2.norm\").alias(\"similarity\"))\n",
        "#tfidf.show(10)\n",
        "print('Done!')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of jobs： 14790\n",
            "\n",
            "the number of each distinct job:\n",
            " DSP engineer                    420\n",
            "computer vision engineer        409\n",
            "FPGA Engineer                   392\n",
            "data-scientist                  387\n",
            "Ruby developer                  385\n",
            "Machine Learning Engineer       379\n",
            "PLC Technician                  379\n",
            "web developer                   377\n",
            "PHP developer                   373\n",
            "python                          372\n",
            "Software Product Manager        372\n",
            "IOS Developer                   372\n",
            "database administrator          370\n",
            "Performance Test Engineer       367\n",
            "NLP engineer                    367\n",
            "Electrical Design Engineer      366\n",
            "Test Automation Engineer        365\n",
            "Embedded Systems Engineer       364\n",
            "statistician                    363\n",
            "computer support specialist     360\n",
            "computer systems analyst        356\n",
            "computer network architect      352\n",
            "Android Developer               348\n",
            "Node js developer               346\n",
            "Python Software Engineer        345\n",
            "Circuit Design Engineer         341\n",
            "Data Analyst                    340\n",
            "Spark Engineer                  339\n",
            "Front End Web Developer         337\n",
            "information security analyst    330\n",
            "Database Engineer               329\n",
            "SQL Developer                   328\n",
            "Golang Software Engineer        324\n",
            "Operations Technician           318\n",
            "JavaScript Developer            308\n",
            "electrical test                 306\n",
            "ARM engineer                    298\n",
            "IT manager                      295\n",
            "Java Software Engineer          290\n",
            "flash Developer                 289\n",
            "Architect                       268\n",
            "software-developer              227\n",
            "Telecommunications Engineer     214\n",
            "software engineer                23\n",
            "Name: job, dtype: int64\n",
            "\n",
            "There are 44 different kinds of jobs in the table.\n",
            "\n",
            "Compute the similarity between jobs and resume...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXO-Isbk6Uj8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0618a0d-bbfb-474d-8912-7ca0307fe6f7"
      },
      "source": [
        "# show Top-20 matched jobs\n",
        "match = tfidf.where('id1 = 0').sort('similarity', ascending=False).where('id2 > 0')\n",
        "top_match = match.limit(20)\n",
        "print('Top 20 matched jobs:')\n",
        "df0.alias(\"a1\").join(top_match.alias(\"a2\"), psf.col(\"a1.id\") == psf.col(\"a2.id2\"))\\\n",
        "    .select(psf.col(\"a1.job\"), \"a1.company\", \"a1.location\", \"a2.similarity\")\\\n",
        "    .sort('similarity', ascending=False).show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 20 matched jobs:\n",
            "+--------------------+------------+--------------------+-------------------+\n",
            "|                 job|     company|            location|         similarity|\n",
            "+--------------------+------------+--------------------+-------------------+\n",
            "|        NLP engineer|       Apple|Seattle, Washingt...|0.10311775333742133|\n",
            "|        NLP engineer|       Apple|Seattle, Washingt...|0.09924933857025435|\n",
            "|        NLP engineer|       Apple|Seattle, Washingt...|0.09585500958328112|\n",
            "|        NLP engineer|       Apple|Seattle, Washingt...|0.09318176036963881|\n",
            "|      data-scientist|    SPECTRUM|    Golden, Colorado|0.09301087086596467|\n",
            "|      Spark Engineer|    SPECTRUM| Englewood, Colorado|0.09301087086596467|\n",
            "|       FPGA Engineer|    SPECTRUM|      Pine, Colorado|0.09301087086596467|\n",
            "|Machine Learning ...|    SPECTRUM|Wheat Ridge, Colo...|0.09301087086596467|\n",
            "|        NLP engineer|    SPECTRUM| Englewood, Colorado|0.09301087086596467|\n",
            "|computer vision e...|       Apple|Seattle, Washingt...|0.09288673114194063|\n",
            "|        NLP engineer|       Apple|Seattle, Washingt...| 0.0917314923617112|\n",
            "|        NLP engineer|       Apple|Seattle, Washingt...|0.08965587265026033|\n",
            "|       FPGA Engineer|       Apple|Seattle, Washingt...|0.08940534574470801|\n",
            "|Machine Learning ...| CyberCoders|West Hollywood, C...|0.08832621014302956|\n",
            "|        NLP engineer|       Apple|Seattle, Washingt...|0.08534446762815676|\n",
            "|        ARM engineer|       Apple|Seattle, Washingt...|0.08287860426417674|\n",
            "|Machine Learning ...| CyberCoders|New York, New Yor...|0.08028920239026649|\n",
            "|      Spark Engineer|Apex Systems|Durham, North Car...|0.07980061305834987|\n",
            "|Machine Learning ...|Apex Systems|Chicago, Illinois...|0.07956921689519444|\n",
            "|Python Software E...|        CHEP|Orlando, Florida ...|0.07925296449486596|\n",
            "+--------------------+------------+--------------------+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9VpHzNJigQE"
      },
      "source": [
        "match = df0.alias(\"a1\").join(match.alias(\"a2\"), psf.col(\"a1.id\") == psf.col(\"a2.id2\"))\\\n",
        "    .select(psf.col(\"a1.job\"), \"a1.company\", \"a1.location\", \"a2.similarity\")\\\n",
        "    .sort('similarity', ascending=False)\n",
        "\n",
        "# create SQL table\n",
        "match.createOrReplaceTempView(\"match\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSfKy7g3n3ez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47bff868-4e8b-4a4c-a039-ec26a7053487"
      },
      "source": [
        "# start SQL query\n",
        "\n",
        "# select jobs in specific location\n",
        "df = spark.sql(\"SELECT * FROM match WHERE location like 'New York City%'\")\n",
        "#df = spark.sql(\"SELECT * FROM match WHERE location like 'San Francisco%'\")\n",
        "df.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                 job|             company|            location|          similarity|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|Python Software E...|    Case Interactive|New York City, Ne...|0.044451186280879226|\n",
            "|JavaScript Developer|    Case Interactive|New York City, Ne...|0.044451186280879226|\n",
            "|              python|    Case Interactive|New York City, Ne...|0.043726558242579094|\n",
            "|      Spark Engineer|        Apex Systems|New York City, Ne...| 0.03862278048733808|\n",
            "|       FPGA Engineer|      Clarapath Inc.|New York City, Ne...|  0.0357684356616758|\n",
            "|Telecommunication...|      Clarapath Inc.|New York City, Ne...|  0.0357684356616758|\n",
            "|           Architect|     Beta Search Inc|New York City, Ne...| 0.03463789785614938|\n",
            "|Python Software E...|     Beta Search Inc|New York City, Ne...| 0.03463789785614938|\n",
            "|Java Software Eng...|     Beta Search Inc|New York City, Ne...| 0.03463789785614938|\n",
            "|        Data Analyst|GRANT PETERS ASSO...|New York City, Ne...| 0.02922028240184282|\n",
            "|          IT manager|  Cloud Destinations|New York City, Ne...|0.027219041309773154|\n",
            "|       web developer|         Alans Group|New York City, Ne...|0.026860833796323997|\n",
            "|   Database Engineer|           Hsgi Inc.|New York City, Ne...| 0.02570415642219932|\n",
            "|  software-developer|   Zeta Global Corp.|New York City, Ne...|0.025186209790556992|\n",
            "|Machine Learning ...|   Zeta Global Corp.|New York City, Ne...|0.025186209790556992|\n",
            "|Telecommunication...|      Clarapath Inc.|New York City, Ne...|0.024633903241568316|\n",
            "|       web developer|      Clarapath Inc.|New York City, Ne...|0.024633903241568316|\n",
            "|Java Software Eng...|            Data Inc|New York City, Ne...|0.023467084139131136|\n",
            "|database administ...|Business Informat...|New York City, Ne...| 0.02113377563799362|\n",
            "|Software Product ...|        Technovision|New York City, Ne...|0.020549938042748243|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLmUQOV-wD2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e9d7fd-18c5-486d-8eb0-a230f877da80"
      },
      "source": [
        "#select specific jobs\n",
        "#df = spark.sql(\"SELECT * FROM match where job = 'computer vision engineer'\")\n",
        "#df = spark.sql(\"SELECT * FROM match where job = 'FPGA Engineer'\")\n",
        "df = spark.sql(\"SELECT * FROM match where job = 'Embedded Systems Engineer'\")\n",
        "\n",
        "df.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                 job|             company|            location|          similarity|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|Embedded Systems ...|Odyssey Systems C...|Lexington, Massac...| 0.05704758412902608|\n",
            "|Embedded Systems ...|  Blue Star Software| Chantilly, Virginia| 0.05577960602990476|\n",
            "|Embedded Systems ...|Southwest Researc...|  San Antonio, Texas| 0.05390175504834788|\n",
            "|Embedded Systems ...|Odyssey Systems C...|Lexington, Massac...|0.053412161939692275|\n",
            "|Embedded Systems ...|Southwest Researc...|  San Antonio, Texas| 0.04839396554216487|\n",
            "|Embedded Systems ...|                PSEG|Hancocks Bridge, ...|0.043340746078394075|\n",
            "|Embedded Systems ...|US ARMY Ground Ve...|WARREN, Michigan ...|0.042577557649579885|\n",
            "|Embedded Systems ...|Southwest Researc...|  San Antonio, Texas| 0.04255284363161199|\n",
            "|Embedded Systems ...|          CVS Health|Monroeville, Penn...| 0.04246768676794431|\n",
            "|Embedded Systems ...|         CyberCoders|Pleasanton, Calif...| 0.04209008786896103|\n",
            "|Embedded Systems ...|              Abbott| Alameda, California| 0.04118547538610749|\n",
            "|Embedded Systems ...|Southwest Researc...|  San Antonio, Texas|0.040773833967869065|\n",
            "|Embedded Systems ...|         CyberCoders|Pasadena, Califor...| 0.04039774506877658|\n",
            "|Embedded Systems ...|     CommScope, Inc.|   Richardson, Texas| 0.03929078620957089|\n",
            "|Embedded Systems ...|Odyssey Systems C...|Lexington, Massac...|0.039120628543782655|\n",
            "|Embedded Systems ...|Southwest Researc...|Oklahoma City, Ok...|0.038651246814334995|\n",
            "|Embedded Systems ...|       Kumu Networks|Sunnyvale, Califo...|0.037811069000351284|\n",
            "|Embedded Systems ...|IntelliPro Group ...|San Jose, California| 0.03650445546180718|\n",
            "|Embedded Systems ...|         CyberCoders|Rochester, New Ha...| 0.03521323446843192|\n",
            "|Embedded Systems ...|         CyberCoders|   Goodyear, Arizona| 0.03521323446843192|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}